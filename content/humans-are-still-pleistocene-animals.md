---
author: Jason Godesky
comments: true
date: 2005-08-27
slug: humans-are-still-pleistocene-animals
title: "Thesis #6: Humans are still Pleistocene animals"
---

In 1833, [Charles Lyell](http://wiki.anthropik.com/Charles_Lyell) introduced the name “[Holocene](http://wiki.anthropik.com/Holocene),” or “Recent Whole,” for our current geological epoch, stretching back only 10 or 12 thousand years. This makes the Holocene an incredibly young geological epoch, the shortest by far. The International Geological Congress in Bologna adopted the term in 1885, and it has been the accepted terminology ever since. The preceding geological epoch was the “last ice age,” the [Pleistocene](http://wiki.anthropik.com/Pleistocene). It lasted for two million years, and while it was marked by significantly advanced glaciation, this was not the unremitting state of affairs. The Pleistocene had regular interglacial periods, during which the weather would turn warmer and the glaciers would temporarily recede–just like today. These interglacials typically lasted an average of 10 - 20 thousand years–just like ours. In short, the “Holocene” is not a new geological epoch, as much as we might think that the grandeur of human civilization’s appearance should be reflected in the ages of the earth. It is a perfectly typical interglacial. The Pleistocene–the “last ice age”–never ended. We’re still in it–we’re simply in a bit of a warm spell.

If anything, our current interglacial is most remarkable for its brevity. If it ended this week and the glaciers returned (and, while [The Day After Tomorrow](http://www.imdb.com/title/tt0319262/) certainly pressed the point too far, [these things do happen very suddenly](http://www.esd.ornl.gov/projects/qen/transit.html)), it would be marked as the shorter side of normal. In fact, [it would have ended some 5,000 years ago](http://www.futurepundit.com/archives/001838.html)–an interglacial of just 5 to 7 thousand years–were it not for the [ecological devastation of the Agricultural Revolution](http://www.scientificamerican.com/article.cfm?chanID=sa006&colID=1&articleID=000ED75C-D366-1212-8F3983414B7F0000). The first farmers were responsible for massive deforestation, and raising huge herds of livestock that polluted the atmosphere with incredible amounts of methane–enough to hold the glaciers in check. For 5,000 years, our civilization has lived on borrowed time, extending our “Holocene” by balancing the earth’s natural cooling trend against our reckless environmental abuse. The Industrial Revolution was not a change in kind, but in scale–a significant increase in our ability to harm the earth’s ecosystems, destroying all semblance of balance that our previous rampages had so precariously struck.

Amazingly, much of the reporting on Ruddiman’s findings, like the [FuturePundit entry](http://www.futurepundit.com/archives/001838.html) cited above, argue that this is evidence that humans *should* try to engineer the planet’s climate. Our agricultural civilization is utterly dependent on the peculiar climate of the Holocene interglacial, this is true. It is a unique product of that climate, and if that climate ends, so will it. In the same fashion, humans are children of the Pleistocene. It is our home, through and through. We have changed far too little in the past 10,000 years to be well-adapted to the epochal changes in our lifestyle that we have seen. We are maladapted to our cultural context. The ecological damage we have done for these past millennia have only extended this state of affairs. Civilization may not be able to survive the end of the Holocene interglacial, but *humanity* certainly can. We are Pleistocene animals.

The Pleistocene was preceded by the Pliocene, an epoch cooler and drier than the preceeding Miocene. Temperatures and rainfall were similar to that of today; in most regions, this meant a colder, drier climate. This was the case in Africa, where jungles shrank and grasslands took their place. Our ancestors were those primates who did not retreat with the jungle, but instead attempted to make their living in the wide, open grasslands. It was in this new challenge that our ancestors, the australopithecines, first defined themselves: by walking upright.

Habitual bipedality is unique in the order primates, though certainly not across the animal kingdom. Australopithecine anatomy shifted to accomodate a vertical, rather than horizontal, alignment. Greater height gave australopithecines the ability to see farther over the grasses, and it gave them a new mode of locomotion in walking.

Walking has unqiue advantages. It is not by any means the fastest mode of transport. Most animals can run faster than humans. However, such locomotion is supported primarily by powerful muscles. This means they tire quickly. Cheetahs can run at over 110 km/hr (70 mph), but it cannot sustain this speed for very long. Most cheetahs will stalk their prey closely, but the final chase will rarely last more than one minute. Walking is very different. Walking does not rely on muscle, but on bone. Walking is a controlled fall, which shifts the body’s weight onto the leg bones, thanks to the locked knee. This means that there is less energy involved in each individual step a bipedal human takes, compared to most quadrupedal animals. Humans may not move as quickly, but they can move more often. The result is an animal that won’t run as quickly, but at the end of the day can cover much more ground.

This tells us something about the changing diet of australopithecus. Many other apes are opportunistic scavengers, and sometimes even hunters. However, this is rarely their primary sustenance. The innovation of walking suggests that australopithecines were relying more on meat than their ancestors had.

The superpredators of Africa had created a harsh Darwinian niche for scavengers, leading to powerful packs of hyenas and flocks of vultures that could easily overpower australopithecines. Instead, australopithecines adopted a strategy of finding the kill site first, getting to it first, grabbing their meat, and retreating before other, more powerful scavengers showed up. Walking upright allowed them to see farther across the grasslands, but a kill site could be *anywhere*. The more ground a scavenger covers in a day, the more likely that scavenger is to stumble upon a kill site. Scavengers don’t necessarily need to be fast–the dead rarely outrun them–they just need to keep moving as long as possible and cover as large a range as possible. The larger their daily range, the higher their chances of finding a kill site. That’s precisely what walking allows for, and australopithecine anatomy was built for nothing quite so perfectly as walking.

We retain those traits even today, which is precisely what makes walking such an important activity. Thomas Jefferson remarked, “Walking is the best possible exercise. Habituate yourself to walk very far.” For more than 99% of our history, humans have been foragers–which meant, more than anything else, walking. While foragers work markedly less than we do, that work consisted almost exclusively of walking: up to four hours every day. The effects of the automobile in the 1950s not only gave us [dating](http://www.digitalhistory.uh.edu/do_history/courtship/flapper.html), it also [destroyed our communities](http://www.usatoday.com/news/health/2003-04-22-walk-cover_x.htm). Resources were no longer grouped together, as walking from place to place became impossible and automobiles became a requirement for existence. Face-to-face interaction [died off](http://www.ransomfellowship.org/R_Putnam.html), and so did the habit of walking–resulting in our current [obesity crisis](http://www.time.com/time/covers/1101040607/). This doesn’t mean that cars and dating are bad–what it means is that we now live in a context to which we are not adapted.

* * *

Two million years ago, the Pliocene became colder and drier still, as the Pleistocene began. The last of these walking australopithecines, *Australopithecus afarensis*, was nearly identical to the first member of our own genus, *Homo habilis*, save in one, crucial regard: *Homo habilis*’s skull was twice the size of the australopithecus’.

Thanks mostly to anthropocentrism, our genus, *Homo*, suffers from what may well be the single most ridiculous defining criteria in all of science: *we use tools*. Of course, we have found tool use in other animals (as we touched on in [thesis #3]({{< relref "humans-are-the-product-of-evolution.md" >}}), and it is entirely likely that various australopithecines used wooden tools at least as complicated as those fashioned by modern-day chimpanzees or crows. Chimpanzees have even been observed with the rare *stone* tool. But the primary reason that this distinction is so laughable as a biological genus is that it is entirely behavioral, and *utterly divorced from biology*!

That is not to say that our tool use isn’t important. Quite the opposite. The explosion in cranial capacity that separates the two contemporary hominid genera seems quite significant. It is very clearly tied to tool use, for while australopithecines may well have fashioned any manner of wooden tools, we only find stone tools associated with *Homo habilis*.

[The Oldowan tool set](http://lithiccastinglab.com/gallery-pages/oldowanstonetools.htm) is the oldest set of technology we know of. It emerged 2.4 million years ago, as the long cooling of the Pliocene–the era of the australopithecines–gave way to the deeper cold of the Pleistocene–the era of our own genus. The making of these stone tools required changes in *Homo habilis*’s brain structures. We find the first evidence for handedness among these earliest members of our genus. We have also learned that handedness, tool use, and *language* are all linked functions in the human brain. Even if *Homo habilis* could not speak, the neurological foundations for it were laid with tool use.

These tools made *Homo habilis* a more efficient scavenger. With choppers and other stone tools, *Homo habilis* could butcher a dead animal more quickly, allowing them to clear out of the kill site more quickly, giving them an evolutionary edge. Yet for all its importance, the Oldowan tool kit changed little in the million years that it was used by *Homo habilis* and the myriad species thrown together into the waste-basket called "*Homo erectus*."[^1] These tools made our genus a far more efficient scavenger. The greater amounts of meat this afforded provided the protein for the explosion in cranial capacity that marked the separation of the hominid genera.

One of the various “*Homo erectus*” species developed the Achulean tool set; others learned how to [use and control fire](http://news.bbc.co.uk/2/hi/science/nature/3670017.stm). Hominids became better scavengers. Now they might have used their weapons to scare off other scavengers, rather than butchering quickly and running from the site. They may have begun to prey upon that gray area ever carnivore treads. No predator will pass up a perfectly good, recent kill–and many scavengers are more than willing to finish off a wounded animal. Or, with sufficient coordination and/or weaponry, a hale and healthy animal. It was in the “*Homo erectus*” period that hominids transitioned from scavengers, to hunters.

Through the 1960s and 1970s, the “Man the Hunter” theory dominated thinking on this topic, explaining human evolution in terms of hunting practices. It was closely linked to thinking on “killer apes,” and a generally Hobbesian view of human nature, painting humans as inherently violent killers. It drew ire from feminists who charged that it neglected the role of females in evolution, while other researchers hoped for evidence to distance human nature from such a grim, violent picture. That theory has declined in recent years, largely due to political correctness.

The feminist critique is rather weak. Since every female has a father, any strong natural selection exerted on one gender will easily cause changes throughout the species, in both genders. Any strong natural selection exerted on women will show up in the male population, as well. There is a much stronger criticism in the analyses of forager diets showing that they rely much more on plants than animals. Richard Lee showed that foragers relied more on plant matter than meat, leading some to refer to “gatherer-hunters” rather than “hunter-gatherers.” However, critics of Lee highlighted his complete reliance on the Ju/’Hoansi, who have an atypical love affair with the mongongo nut. More cross-cultural studies[^2] found that forager diets correlated to latitude: foragers closer to the equator ate more plants, foragers closer to the poles ate more meat. They also found significantly more meat than Lee: near 100% for such polar extremes as the Inuit, but only 14% of forager cultures in total got even half of their diet from plants. Despite this solid refutation, much is still made of Lee’s findings. An emerging concensus supports this “gatherer-hunter” model, though nearly all arguments for it are based on political correctness.

For the opponents of the “Man the Hunter” theory, acquiescing that hunting was an important part of human evolution is to normalize and excuse violence. It rests on an idea that is very old in Hinduism and Buddhism, which has only in recent decades formed vegetarian thought in the West: the idea of meat-eating as an inherently violent act. The presumption of this argument is that violence is only violence if enacted upon animals; that one cannot by violent towards plants. There is an assumption in this that while animals are alive, plants really aren’t. This is also a very old idea. The name “animal” derives from the Latin *animus* or spirit, because animals are animated–moved by a spirit–while plants are not. Even in shamanic and animistic schemes, animal life is often elevated above plant life.

The underpinnings for this belief have little basis in fact. As animals, animals are closer to us, and thus enjoy some special concern from us for their proximity. At its base, this is simply one more concentric circle in the widening ripples of anthropocentrism. As Giulianna Lamanna highlighted in her article, “The Hypocrisy of Vegetarianism,” there is even some intriguing indications of the possibility that plants may even *feel* in some strange way. Violence against a carrot is every bit as much violence, as violence against a cow.

Yet the proponents of “Man the Hunter” have predicated it upon an inherently evil and violent human nature; its detractors have predicated it upon an inherently good and gentle human nature. Both are idealized and misguided. We do not think of other predators as evil or violent, do we? Do we conceive of lions, or sharks, or bears, or spiders in such ways? Predators are important parts of the natural world. The return of the wolves to Yellowstone [restored the park’s ecology](http://scientificamerican.com/article.cfm?articleID=00076914-0667-10AA-84B183414B7F0000) which had been thrown out of balance by the predator’s departure.

We have already seen that both views of humans as good and humans as evil are overly simplistic ([thesis #5]({{< relref "humans-are-neither-good-nor-evil.md">}})). The issue of humanity and hunting is a fine example of such an issue that cuts both ways. Tracking requires careful observation, but even that alone is insufficient. Careful observation yields only an assemblage of data points. The tracker must assemble those points into a narrative, to weave a story around that data that not only says where the animal was and what it did, but predicts where it is going, as well. The needs of the tracker provide the natural selective pressure for human cognition as we know it.

But hunting is never a sure thing. Sometimes you bag yourself a big, juicy kill, and sometimes you come home empty-handed. Skill has a lot to do with it–but so does luck. Among foragers, it’s been calculated that on any given hunt, a hunter only has a 25% chance of making a kill. Yet our ancestors not only derived most of their protein from meat, they derived most of their daily energy from meat, as well. How did they do this, if they only ate one day out of four? While the probability that one hunter will fail on a given day might be 0.75, the probability that four hunters that all go out on the same day will all fail to catch something is 0.316. In other words, if four hunters all agree to share whatever they kill between them, then there is generally a 68% chance that all four of them will eat that day–where alone, their chances drop to 25%.

The risks involved in hunting made cooperation an important human strategy. Unlike other primates, our bonds formed into small, open, cooperative, *egalitarian* groups. The adoption of human society to mitigate hunting risks emphasized that any hunter could be the one bringing home dinner that night, and ultimately the conviction that everyone has value to the group. Sharing evolved not as a virtue, but as a necessity. In forager groups today, sharing is not considered “nice,” it’s simply expected as a social baseline, and as a requirement for survival.

Hunting inhabits a morally ambiguous position, then. The act itself is violent, yet its risks gave us the very notion of society and its attendant virtues of sharing, cooperation, and compassion–the very same virtues vegetarians seek to promote by denying that very thing that created them. The risks of hunting instilled in our ancestors their first sense of wonder and reverence. They saw the animals they killed not as trophies as we might, but as sacrifices necessary for survival. They worshipped the animals they consumed, using the narrative cognition tracking bestowed upon them to yield the first philosophy and religion humans would ever have. As shamans charted the expanses of human consciousness, art, music and science followed. The first hominids made their lives as communal scavengers, but as they learned to hunt, they became human.

* * *

But man does not live by meat alone, but by every nut, berry, tuber and leafy green that comes from the hand of woman. While the supposition that foragers were “gatherer-hunters” is little more than political correctness projecting itself back into our evolutionary history, neither can we ignore the importance of gathered foodstuffs. Foragers did divide labor roughly along gender lines, with males usually taking up most of the hunting, for obvious, biological reasons. Even though it was hunting that provided not only the protein our bodies required, but also most of the energy we used, it would be a mistake to discount the role of women.

Besides energy and protein, our bodies require smaller amounts of vital micronutrients. We do not need them in large quantities, but we do very much need them. Without sufficient vitamin A, children go blind. Insufficient vitamin D leads to rickets. If you don’t get enough vitamin C, you’ll come down with a case of scurvy. Wild edible plants provided these in abundances our modern domesticates cannot hope to match. Two cups of dandelion leaves contain more vitamin C than four glasses of orange juice; dandelions have more beta carotine than carrots, and more potassium than potatoes or spinach–alongside healthy doses of iron and copper. You’ll find wild edibles replete with quantities of vitamins, minerals, omega 3 fatty acids and all manner of other nutrients that float in our public consciousness precisely because our modern diet so clearly lacks them.

The line between food and medicine was not so clear, either. Common, broadleaf plantain is, along with dandelion, probably one of the most nutritious plant in the world, but plantain is also a powerful pain-killer, as well as having anti-toxic, anti-microbial, and anti-inflammatory properties. When ingested, it is a demulcent, a diuretic, and an expectorant. By the same token, dandelions can be used as a general tonic that helps strengthen the liver, gall bladder, pancreas, spleen, stomach, and intestines. They improve bile flow and reduce inflammation in cases of hepatitis and cirrhosis.

Women did not simply gather side dishes crucial to nutrition and survival; they provided medicines that not only cured sickness, but improved health, as well. Where male hunters cultivated spacial perception and risk-sharing strategies, could it have been the needs of female gatherers that gave us much of our abilities for memory and memorization?

* * *

As Paleolithic foragers, humans were beginnning to develop a new strategy to survive the Pleistocene. Many animals learn a great deal, and use this to supplement their instincts. Orangutans have identifiable cultures, and similar observations have been made of chimpanzees. Humans took this to an extreme, with very few inborn instincts. Instead, our brain became hard-wired not for any specific behavior set, but for recieving culture. In the acculturation process, we learn the rules and taboos of the culture we are born into, and incorporate them on a very deep level. Things that disgust us, for example–particularly food and sex taboos–are usually very arbitrary, yet we feel them so deeply that they are often mistaken for natural, universal truths.

We might think of this innovation in similar terms to the early history of computing. Early computers, or Turing machines, were made to perform a specific task. The innovations of von Neumann, Simon and others led to computers that were made to run arbitrary programs. Most animals have a much larger repository of instincts than we do, and learn much less. This leads to species-wide behavior patterns. Humans, on the other hand, owe much more of their behavior to culture than instinct. This means that culture can provide another layer of adaptation that can change much more quickly than evolution. It gives humans a competitive edge, by allowing us to adapt to any new environment with incredible speed and ease. When combined with our omnivorism opening a much wider array of possible foods, humans have thus become very possibly the most adaptable species on the planet.

Most animals, when confronted by fire, have a natural instinct to run away. At some point, long ago in our history, that instinct was stalled by our acculturation, and rather than run from it, some human actually went towards it, and brought it back under her own control. In time, we even learned how to start our own fires, yet the turning point of that first human to run *towards* the fire remains one of the most pivotal moments in our history. The Greeks immortalized that event in the myth of Prometheus, and the mythology of the San point to it as the turning point of our species:

> Kaang gathered all the people and animals about him. He instructed them to live together peacefully. Then he turned to the men and women and warned them not to build any fires or a great evil would befall them. They gave their word and Kaang left to where he could watch his world secretly.

> As evening approached the sun began to sink beneath the horizon. The people and animals stood watching this phenomenon, but when the sun disappeared fear entered the hearts of the people. They could no longer see each other as they lacked the eyes of the animals which were capable of seeing in the dark. They lacked the warm fur of the animals also and soon grew cold. In desperation one man suggested that they build a fire to keep warm. Forgetting Kaang’s warning they disobeyed him. They soon grew warm and were once again able to see each other.

> However the fire frightened the animals. They fled to the caves and mountains and ever since the people broke Kaang’s command people have not been able to communicate with animals. Now fear has replaced the seat friendship once held between the two groups.

Humans spread out of Africa, into Asia and Europe. The ice age lowered the water levels, revealing the Bering Land Bridge, which humans followed into the Americas. The lower water levels made the islands of Indonesia and Micronesia larger, and the water between them smaller. Humans hopped from island to island in ancient canoes, until eventually they reached Australia. In these new environments, humans often relied more heavily on meat, at least at first, as they learned the new flora of these strange lands, what was safe to eat, and what was poisonous.

Until recently, the term “Holocene Extinction” referred to a rather minor spate of extinction which took place at the beginning of the Holocene, with the end of the megafauna–woolly mammoths, North American horses, sabertooth cats, and other large mammals. This occurred at the beginning of the Holocene, as humans were first moving into many new environments, like the Americas and Australia. This has led to a long-standing debate between “overkill” and “overchill.” Were the megafauna wiped out by climate change? Or by rapacious, brutal bands of overhunting human foragers? [Both sides](http://www.sciencedaily.com/releases/2001/10/011025072315.htm) have their evidence, of course.

Nor is this merely an academic argument without repercussion for the present. The “overkill” theory is routinely cited by some groups as if it were already a proven fact, and used as evidence that humans are an inherently destructive species. So we needn’t worry ourselves with the environmental destruction we wreak. We can’t help it. It’s our nature.

As you might expect, the truth lies somewhere between overkill and overchill. Human populations were almost certainly too small to wreak such havoc all by themselves, and the same climate changes that opened the way for humans into Australia and the Americas also had to affect the other large mammals living across the globe. Even more instructive, however, is the modern case of the [wolves of Yellowstone](http://scientificamerican.com/article.cfm?chanID=sa006&colID=1&articleID=00076914-0667-10AA-84B183414B7F0000). Alpha predators–like wolves, and like humans–play important, keystone roles in any ecology. The introduction of a new alpha predator can have dramatic effects, even causing cascades of extinction. This is not necessarily because the alpha predators overhunt or are even in the least bit maladaptive; this is simply the nature of alpha predators and how they relate in any given ecology. When humans came to Australia and the Americas, they were as harmless as wolves, lions, or any other big mammalian predator. Their presence caused cascades of changes throughout the ecosystem. Given that it was also a period of major climate change, a great number of species that were already under stress adapting to the new climate were tipped over the edge into extinction by the further ecological changes created by the adaptation of a new alpha predator. Our ancestors were hardly noble savages; but neither were they bloodthirsty killers bent on the destruction of all life on earth. They were animals, like any other.

* * *

In the Upper Paleolithic, we see a “revolution” leading to what paleoanthropologists sometimes refer to as “behavioral modernity.” There is a good deal of misinformation all around on this point, so let me first address this concept of “modernity.” Like the waste-basket of *Homo erectus*, paleoanthropologists have shoe-horned many different species into the category of “anatomically modern *Homo sapiens*” not based on fossil evidence, but because of their age. The alternative would be to recognize that human evolution was not a process of unilineal evolution–that it was not a tree, but a “bush.” Though this conclusion has become inescapable to most paleoanthropologists today, the categorizations of their predecessors who were not so enlightened often remain.

This has led to some startlement among paleoanthropologists, as we see “anatomically modern” humans, but without evincing any sign of the things we define ourselves by: art, religion, philosophy, etc. So, many have split “modernity” into anatomical and behavioral aspects. This is a false dilemna born not only of the rough shoe-horning of evidence already discussed, but also of the “revolution” idea born of Eurocentrism.

In Europe, the Upper Paleolithic truly is a “revolution.” We have cave art, sculptures, musical instruments, evidence of arithmetic and astronomy all appearing at once. This led many paleoanthropologists to think that “modern behavior” was a package deal, that there was some kind of genetic switch that allowed them all to fllower at once.

In Africa, however, we see each of these various elements accrue over time. They do not appear all at once, as in Europe. The conclusion is simple, and straightforward: “behaviorally modern” humans came out of Africa. This is the same “out of Africa” hypothesis that has won almost unanimous support over the multiregional hypothesis that has so long been the bulwark of racists and pseudo-scientists. If we look only at the European evidence, then, we have a “revolution”–but only because these new, African tribes arrived at a given time, practicing all of their culture at once.

Yet, all of these cultural phenomena that we define ourselves by *do* have a common origin, in shamanism. [David Lewis-Williams](http://www.amazon.com/exec/obidos/ASIN/0500284652/anthropik-20) is at his most convincing when he shows the underpinnings of shamanism in human neurology and psychology, and how rock art is an expression of that. Michael Winkelman has written a great deal on the evolutionary adaptations of shamanism. Both show how important shamanism was as an adaptation to the Pleistocene environment we evolved in, not only to reconcile the workings of our inner worlds to the world we live in, but also as a touchstone of community life and social function, an integrative function for the psychologically aberrant, and a healing function for the individual and the community.

Shamans most often induced altered states of consciousness through repetitive sound and motion–song and dance. Their visions provided the philosophy and world-view of their tribes, giving rise to the first religion and philosophy. Often, shamanic rituals were tied to the motions of the celestial bodies–and the first evidence we have of arithmetic is a “counting stick” cut off in sets of 28, most likely tracking the phases of the moon. Shamans were ethnobotanists of the highest order, and were willing to experiment even with the spirit world, so in some sense, we might even trace the first glimmerings of science to them, as well. “Behavioral modernity” goes back to the Upper Paleolithic, a gift from the shaman, and that unique adaptation to the Pleistocene that first tried to map the universe in our own minds.

* * *

The Pleistocene lasted for two million years–the same two million years that saw the rise of our genus. Like all animals, we are products of evolution, adapted to a specific niche. Our niche was the Pleistocene. The Holocene has been far too short for any significant amount of adaptation to occur, and how maladapted we are to our current lifestyle should be obvious. The effects of not walking as often on our health has already been touched upon. The loss of the shaman’s role has led to the marginalization of people once well integrated into society, and the loss of tribal society has been catastrophic in other ways which we will explore in future theses. For the moment, I would like to turn to just one arena in which the Holocene has proven the bane of our species: health.

The Agricultural Revolution was a massive change in diet. Where once we had gained the majority of our energy from animal proteins, and our food came from hundreds of different species, the Neolithic saw an utter reliance on less than a dozen different species, with the majority of our energy now coming from carbohydrates. Even today, more than 50% of the American diet comes from just three plants–wheat, rice and potatoes.

[Ben Balzer’s introduction to the “Paleolithic Diet”](http://www.earth360.com/diet_paleodiet_balzer.html) provides a great deal of wonderful information on the nutritional deficits that agriculture has given us, but for now I will simply quote his analysis of grains:

These advantages made it much easier to store and transport food. We could more easily store food for winter, and for nomads and travelers to carry supplies. Food storage also enabled surpluses to be stored, and this in turn made it possible to free some people from food gathering to become specialists in other activities, such as builders, warriors and rulers. This in turn set us on the course to modern day civilization. Despite these advantages, our genes were never developed with grains, beans and potatoes and were not in tune with them, and still are not. Man soon improved further on these advances- by farming plants and animals.

As Belzar points out, grains are powerful packets of energy. The plants wrap their seeds in carbohydrates, to give them the energy to grow. Co-evolution has struck a balance between the needs of plants and animals: animals eat seeds and fruits, and in return, they help spread the seeds. Key here, from the plant’s point of view, is that the seed not be destroyed, or else its part of the deal has been eliminated. Animals adapted to certain plant types will be able to gain nutrition from the wrapping around the seed, and those plants will generally not become toxic to those animals (or else their seeds won’t travel very far). However, the seed itself is often quite toxic, to make sure the animals don’t eat them.

Humans are very well adapted to eating any number of such fruits, nuts and so forth. Grains, however, are not on that list. Hominids have experimented with eating grains in the past. The only hominids ever adapted to that diet were the genus Paranthropus, once classified as the “robust” branch of the Australopithecines. These are, at best, distantly related great-uncles to our own species. We are not descended from them, and have not inherited the various enzymes and chemicals they required to make use of grains.

Grains are quite toxic when eaten raw, but cooking can render them edible. Even then, they are of substantially lower nutritional quality than almost any other edible plant. They contain little more than carbohydrates–an energy source our body can surely make use of, but it is not our bodies’ favored source of energy. We are better adapted to the use of protein for energy. Grains also include a number of “anti-nutrients,” such as lectins, which can have as wide-spread an effect through the body as hormones, but because they are foreign (and maladapted) to the human body, cause effects that are unpredictable and often deleterious. It may well be because of lectins that the first study of correlative cancer causes, performed by Stanislaw Tanchou in 1843, remains the most accurate. [This page from paleodiet.com](http://www.paleodiet.com/cancer.txt) reviews much of the evidence for grains’ implication in cancer. It includes:

> Stanislaw Tanchou “….gave the first formula for predicting cancer risk. It was based on grain consumption and was found to accurately calculate cancer rates in major European cities. The more grain consumed, the greater the rate of cancer.” Tanchou’s paper was delivered to the Paris Medical Society in 1843. He also postulated that cancer would likewise never be found in hunter-gatherer populations. This began a search among the populations of hunter-gatherers known to missionary doctors and explorers. This search continued until WWII when the last wild humans were “civilized” in the Arctic and Australia. No cases of cancer were ever found within these populations, although after they adopted the diet of civilization, it became common.

The mechanism is not difficult to imagine. Cancer cells appear in every healthy human body with some frequency, but the immune system identifies them as foreign and destroys them. Lectins–and generally poor nutrition–suppress the immune response, allowing more cancer cells to survive and become tumors.

Grains, beans and potatoes also have many enzyme blockers that shut down significant parts of the human digestive system. The most common type are protease inhibitors, which block the enzyme protease, which is required in the digestion of protein. Most of these are broken down in cooking, but not all.

Beyond the negative health effects of these “Neolithic foods,” they are incredibly poor in the various other nutrients humans need, which were provided in abundance by our forager lifestyle. This is the very reason that humans in industrialized societies so often need dietary and vitamin supplements–our diet does not provide the nutrition we need, the nutrition our bodies evolved to expect.

Why then is bread called “the staff of life”? Simply because it is a staple food. Eaten in sufficient quantities, it can keep us alive–as it has kept generations of civilized people alive. It will keep us alive, for a short, sickly life. The finds at Dickson’s Mounds showed the effects of agriculture. The literal children of six foot tall foragers that lived into their 60s or 70s with perfect health, would die of malnutrition or disease in their 20s or 30s, barely reaching five feet. The concentrated populations, proximity to animals (allowing germs to jump the species barrier), and heavy trade of agricultural life allowed for the rise of disease as we know it–this was one of Jared Diamond’s main points in his indictment of agriculture as “the worst mistake in the history of the human race.” But malnutrition and starvation also became fixtures of human life then. As a general rule, only farmers ever starve.

Realizing this, some have now started calling bread “the staff of death.” Steve Brill calls the average American “overfed and malnourished,” an idea that evokes the scientific idea of “affluent malnutrition.” We maintain our lives–and even our obesity–by eating enormous quantities, but what we eat is so nutritionally bankrupt that even then we are only barely getting the basic requirements for survival.

Our civilization does need the Holocene. The grains it is so utterly dependent on are temperamental crops that can only tolerate the most minute climatic fluctuations. Humans, however, are animals of the Pleistocene. The Pleistocene was our home, it is what we are adapted to. The short 10 millennia of the Holocene has not given us sufficient time to adapt to our modern lives. Those lives are very nearly contradictions of the environment we evolved in: hierarchical, rather than egalitarian; carbohydrate-based, rather than protein-based; sedentary, rather than nomadic; plant-based, rather than animal-based; specialized, rather than generalized; regimented, rather than free-form; marginalized, rather than integrated. It is the very definition of dehumanizing.

There is a glimmer of hope, though. The Holocene is not the geological epoch we glorified it as. It is merely an interglacial; an interglacial due to end any time now. The Pleistocene will return. It’s almost time to go home.

[^1]: At the University of Pittsburgh, I had the great fortune to take one of Dr. Jeffrey H. Schwartz’s workshop courses, where we had the opportunity to examine many specimens of *H. erectus* closely. Schwartz demonstrated to my satisfaction that *H. erectus* is, in fact, at least a dozen distinct human species. Why, then, have they been lumped together? As Schwartz [was quoted by the BBC](http://news.bbc.co.uk/1/hi/sci/tech/3857113.stm), “Palaeoanthropologists often have this assumption that every hominid found from that time period is a *H. erectus*. They group hominids not on the basis of what they look like, but the time when they lived, which is totally unfounded. There is a tradition of confusing diversity with variation.” So, the myth of “evolution as progress,” as discussed and dismissed in [thesis #2]({{< relref "evolution-is-the-result-of-diversity.md" >}}), led paleoanthropologists to divide human evolution into stages in a story of progress to our final, ideal form. Then, fossils were fit into a given stage not because of morphological differences, but based on their dating and how they would fit into our progression. In fact, as we know, evolution engenders *diversity*, not progress–so the more complicated, diverse history laid out by the actual fossil evidence is far more realistic than the picture of lineal progress painted previously.

[^2]: Cordain, et al, 2000. “[Plant-Animal Subsistence Ratios and Macronutrient Energy Estimations in Worldwide Hunter-Gatherer Diets](http://media.anthropik.com/pdf/cordain2000.pdf)” *American Journal of Clinical Nutrition*, 71:682-692
